{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanskrit to Text-Fabric\n",
    "This notebook converts a\n",
    "[Sanskrit text resource](https://etcbc.github.io/text-fabric-data/features/sanskrit/dcs/0_home.html), suggested by [Tyler Neill (Leipzig)](http://www.gko.uni-leipzig.de/indologie-zaw/institut/mitarbeiter-in-forschungsprojekten/tyler-graham-neill.html)\n",
    "into Text-Fabric.\n",
    "\n",
    "## Disclaimer\n",
    "Note that this corpus has some problems.\n",
    "\n",
    "**This is only a demonstrator**\n",
    "\n",
    "The purpose is to set op a text-fabric processing pipeline for this kind of resource.\n",
    "\n",
    "## Structure of the corpus\n",
    "The resource is a collection of files, which for now we treat as *books*.\n",
    "The lines in the files are numbered with two hierarchical numbers.\n",
    "We treat the principal number as a *chapter* number and the other as a *verse* number.\n",
    "\n",
    "## Modeling the data in Text-Fabric\n",
    "Slots correspond to letters, and we skip the spaces, but not without having them leave a trail.\n",
    "So the primary data is considered to be a consecutive stream of non-white characters.\n",
    "\n",
    "Yet we also have a node type word, whose text values are the words as delimited in the original text.\n",
    "\n",
    "We give the letter nodes a feature `char`, which contains the unicode character at that position.\n",
    "Besides `char`, letter nodes also have a feature `trailer`, which is a space if that letter is at the end of an\n",
    "original word, and the empty string in other cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "We import some of the most generic Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re, collections\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the import of the Text-Fabric library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tf.fabric import Fabric\n",
    "from tf.timestamp import Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use variables to point to the input repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REPO = os.path.expanduser('~/github/sanskrit_text_dcs')\n",
    "TEXT_DIR = '{}/corpora'.format(REPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a dataset in the `text-fabric-data` repository, where it sits alongside the \n",
    "Hebrew Bible and the Greek New Testament. \n",
    "\n",
    "This place is used only temporarily.\n",
    "Once it has decided to pursue this path further, we will seek a destination under the control of\n",
    "the researchers that actually work with this corpus.\n",
    "\n",
    "We supply a location to the `TF()` call where currently is no dataset.\n",
    "TF will warn about missing grid features (`otype`, `oslots`), but that will not concern us,\n",
    "because in the following cells we will construct these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.7\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "0 features found and 0 ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s Grid feature \"otype\" not found in\n",
      "/Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "  0.00s Grid feature \"oslots\" not found in\n",
      "/Users/dirk/github/text-fabric-data/sanskrit/dcs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.01s Grid feature \"otext\" not found. Working without Text-API\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tm = Timestamp()\n",
    "TF = Fabric(locations=['~/github/text-fabric-data'], modules=['sanskrit/dcs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the corpus\n",
    "\n",
    "We read each file in turn. Extracting information is deferred to `readText()` below.\n",
    "When errors arise, we collect them here. Only a handful of errors will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors = collections.defaultdict(list)\n",
    "slotNum = 0\n",
    "nodeFeatures = collections.defaultdict(dict)\n",
    "edgeFeatures = collections.defaultdict(dict)\n",
    "\n",
    "CHAR = 'char'\n",
    "TRAILER='trailer'\n",
    "WORD = 'word'\n",
    "BOOK = 'book'\n",
    "SECTION = 'chapter'\n",
    "LINE = 'verse'\n",
    "nodes = collections.defaultdict(list)\n",
    "\n",
    "def showErrorSummary():\n",
    "    errorTexts = sorted(errors.keys())[0:3]\n",
    "    for errorText in errorTexts:\n",
    "        for error in errors[errorText][0:3]:\n",
    "            print(error)\n",
    "\n",
    "def readCorpus():\n",
    "    tm.indent(reset=True)\n",
    "    tm.info('Reading corpus')\n",
    "    os.chdir(TEXT_DIR)\n",
    "    errors.clear()\n",
    "    nodeFeatures.clear()\n",
    "    edgeFeatures.clear()\n",
    "    nodes.clear()\n",
    "    global slotNum\n",
    "    slotNum = 0\n",
    "    textFiles = sorted(os.path.splitext(f)[0] for f in glob('*.txt'))\n",
    "    print('{} texts'.format(len(textFiles)))\n",
    "    for textFile in textFiles:\n",
    "        readText(textFile)\n",
    "    if len(errors):\n",
    "        print('There were {} errors'.format(sum(len(errors[textFile]) for textFile in errors)))\n",
    "        showErrorSummary()\n",
    "    else:\n",
    "        print('No errors')\n",
    "    print('''\n",
    "{} slots\n",
    "{} words in source\n",
    "{} lines\n",
    "{} sections\n",
    "{} books\n",
    "'''.format(\n",
    "        slotNum,\n",
    "        len(nodes[WORD]),\n",
    "        len(nodes[LINE]),\n",
    "        len(nodes[SECTION]),\n",
    "        len(nodes[BOOK]),\n",
    "    ))\n",
    "    tm.info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a few regular expressions to scan the data lines of the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linePat = re.compile('^\\s*([^\\/]*)\\/+\\s*\\(([^.)]+).([^)]+)\\)\\s*')\n",
    "emptyLinePat = re.compile('^\\s*$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function, `readText()` reads a text file line by line, splits text and numbering, splits\n",
    "the words of the text into characters.\n",
    "It will construct dictionaries that correspond to the features that we will write out later into the new\n",
    "Text-Fabric resource.\n",
    "\n",
    "In this function the slot numbers (for the letters) will be determined.\n",
    "We postpone the numbering of non-slot nodes.\n",
    "We only collect lists of non-slot node and their feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readText(textFile):\n",
    "    global slotNum\n",
    "    with open('{}.txt'.format(textFile)) as f:\n",
    "        bookName = textFile\n",
    "        bookStart = slotNum + 1\n",
    "        curSection = None\n",
    "        sectionStart = slotNum + 1\n",
    "        for (n, line) in enumerate(f):\n",
    "            lineStart = slotNum + 1\n",
    "            line = line.rstrip('\\n')\n",
    "            if emptyLinePat.match(line): continue\n",
    "            match = linePat.match(line)\n",
    "            if not match:\n",
    "                errors[textFile].append('{}:{} - unexpected line\\n\\t{}\\n'.format(textFile, n + 1, line))\n",
    "                continue\n",
    "            text = match.group(1).rstrip()\n",
    "            sectionNr = match.group(2)\n",
    "            if sectionNr == None:\n",
    "                break\n",
    "            if curSection != sectionNr:\n",
    "                if curSection != None:\n",
    "                    sectionEnd = slotNum\n",
    "                    nodes[SECTION].append((sectionStart, sectionEnd, {SECTION: curSection, BOOK: bookName}))\n",
    "                curSection = sectionNr\n",
    "                sectionStart = slotNum + 1                \n",
    "            lineNr = match.group(3)\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                wordStart = slotNum + 1\n",
    "                for letter in word:\n",
    "                    slotNum += 1                 \n",
    "                    nodeFeatures[CHAR][slotNum] = letter\n",
    "                    nodeFeatures[TRAILER][slotNum] = ''\n",
    "                wordEnd = slotNum\n",
    "                nodeFeatures[TRAILER][slotNum] = ' '\n",
    "                nodes[WORD].append((wordStart, wordEnd, {WORD: word}))\n",
    "            lineEnd = slotNum\n",
    "            nodes[LINE].append((lineStart, lineEnd, {LINE: lineNr, SECTION: curSection, BOOK: bookName}))\n",
    "        sectionEnd = slotNum\n",
    "        if curSection == None:\n",
    "            print('Empty book {}'.format(textFile))\n",
    "        else:    \n",
    "            nodes[SECTION].append((sectionStart, sectionEnd, {SECTION: curSection, BOOK: bookName}))\n",
    "            bookEnd = slotNum\n",
    "            nodes[BOOK].append((bookStart, bookEnd, {BOOK: bookName}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion notes\n",
    "\n",
    "190 texts\n",
    "\n",
    "### Empty Texts\n",
    "\n",
    "```\n",
    "Empty book Gřḍhārthaprakāśaka\n",
    "Empty book Kaulāvalīnirṇaya\n",
    "Empty book Mṛgendraṭīkā\n",
    "Empty book Nyāyacandrikāpaṇjikā\n",
    "Empty book Śārṅgadharasaṃhitādīpikā\n",
    "Empty book Tantrasaṃgraha\n",
    "Empty book Tantrāloka\n",
    "```\n",
    "\n",
    "**Action taken**\n",
    "\n",
    "Skipped them altogether\n",
    "\n",
    "### Irregular lines\n",
    "\n",
    "```\n",
    "There were 3 errors\n",
    "Agastīyaratnaparīkṣā:55 - unexpected line\n",
    "\t\t[... auein Vers / Satzjh] // (27.2)hariśvetaṃ tathā vaṃśe pītaśvetaṃ ca śūkare // (28.1)\n",
    "\n",
    "Gokarṇapurāṇasāraḥ:185 - unexpected line\n",
    "\t\titi śrīskānde gokarṇakhaṇḍe śrīgokarṇamāhātmye sāroddhāre prathamo 'dhyāyaḥ / // (88.1)\n",
    "\n",
    "Rasādhyāya:130 - unexpected line\n",
    "\t\t[... auein Vers / Satzjh] // (64.2)tāmrāt sūtaṃ rasāttāmraṃ pātanāya pṛthakkṛtam / (65.1)\n",
    "```\n",
    "        \n",
    "**Action taken**\n",
    "\n",
    "Case 1 and 3: inserted a newline, changed the first / into an `~`\n",
    "\n",
    "Case 2: removed the `//`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we call the reading functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Reading corpus\n",
      "190 texts\n",
      "Empty book Gřḍhārthaprakāśaka\n",
      "Empty book Kaulāvalīnirṇaya\n",
      "Empty book Mṛgendraṭīkā\n",
      "Empty book Nyāyacandrikāpaṇjikā\n",
      "Empty book Śārṅgadharasaṃhitādīpikā\n",
      "Empty book Tantrasaṃgraha\n",
      "Empty book Tantrāloka\n",
      "No errors\n",
      "\n",
      "1161379 slots\n",
      "136409 words in source\n",
      "25729 lines\n",
      "13010 sections\n",
      "183 books\n",
      "\n",
      "  1.40s Done\n"
     ]
    }
   ],
   "source": [
    "readCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "We supply the necessary metadata for the new features.\n",
    "We also have a few generic fields that will be added to all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metaData = {\n",
    "    '': dict(\n",
    "        createdBy='Tylor Neill and Dirk Roorda',\n",
    "        name='Sanskrit_Corpus_DCS',\n",
    "        title='Sanskrit Corpus',\n",
    "        provenance='[DCS](http://kjc-fs-cluster.kjc.uni-heidelberg.de/dcs/index.php)',\n",
    "        description='DCS, the Digital Corpus of Sanskrit, is a searchable collection of lemmatized Sanskrit texts. It offers free internet access to a part of the database of the linguistic program SanskritTagger, which has been under constant development since 1999.'\n",
    "    ),\n",
    "    'otext': {\n",
    "        'sectionFeatures': ','.join((BOOK, SECTION, LINE)),\n",
    "        'sectionTypes': ','.join((BOOK, SECTION, LINE)),\n",
    "        'fmt:text-orig-full': '{{{}}}'.format(CHAR),\n",
    "        'fmt:text-orig-segmented': '{{{}}}{{{}}}'.format(CHAR, TRAILER),\n",
    "    },\n",
    "    'otype': {\n",
    "        'valueType': 'str',        \n",
    "    },\n",
    "    'oslots': {\n",
    "        'valueType': 'str',\n",
    "    },\n",
    "    'book@sa': {\n",
    "        'valueType': 'str',\n",
    "        'language': 'Saṃskṛtam',\n",
    "        'languageCode': 'sa',\n",
    "        'languageEnglish': 'sanskrit',\n",
    "    },\n",
    "    'trailer': {\n",
    "        'valueType': 'str',\n",
    "    }\n",
    "}\n",
    "nodeFeatures['book@sa'] = nodeFeatures[BOOK]\n",
    "\n",
    "for (sectionType) in (CHAR, WORD, LINE, SECTION, BOOK):\n",
    "    metaData.setdefault(sectionType, {})['valueType'] = 'int' if sectionType in {LINE, SECTION} else 'str'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add features that contain the frequency and rank of all words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeStatistics():\n",
    "    tm.info('Computing statistics')\n",
    "    wstats = {\n",
    "        'freq': collections.Counter(),\n",
    "        'rank': {},\n",
    "    }\n",
    "    word = {}\n",
    "\n",
    "    words = [n[0] for n in nodeFeatures['otype'].items() if n[1] == WORD]\n",
    "\n",
    "    for w in words:\n",
    "        occ = nodeFeatures[WORD][w]\n",
    "        wstats['freq'][occ] += 1\n",
    "    rank = -1\n",
    "    prev_n = -1\n",
    "    amount = 1\n",
    "    for (x, n) in sorted(wstats['freq'].items(), key=lambda y: (-y[1], y[0])):\n",
    "        if n == prev_n:\n",
    "            amount += 1\n",
    "        else:\n",
    "            rank += amount\n",
    "            amount = 1\n",
    "        prev_n = n\n",
    "        wstats['rank'][x] = rank\n",
    "    tm.info('Done')\n",
    "\n",
    "    tm.info('Adding statistics as features')\n",
    "    occFeatures = {}\n",
    "    for ft in ('freq', 'rank'):\n",
    "        occFeatures[ft] = {}\n",
    "        metaData.setdefault(ft, {})['valueType'] = 'int'\n",
    "\n",
    "    for w in words:\n",
    "        occ = nodeFeatures[WORD][w]\n",
    "        for ft in ['freq', 'rank']:\n",
    "            occFeatures[ft][w] = str(wstats[ft][occ])\n",
    "\n",
    "    nodeFeatures.update(occFeatures)\n",
    "    tm.info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling into TF\n",
    "\n",
    "Here we put all bits together:\n",
    "\n",
    "1. the features for letters and words\n",
    "2. the nodes for words, verses, chapters and books\n",
    "3. the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeTextFabric():\n",
    "    tm.indent(reset=True)\n",
    "    tm.info('Generating text-fabric dataset')\n",
    "    nodeFeatures['otype'] = dict((n, 'letter') for n in range(1, slotNum + 1))\n",
    "    nodeNum = slotNum\n",
    "    for (nodeType) in (WORD, LINE, SECTION, BOOK):\n",
    "        for (start, end, feats) in nodes[nodeType]:\n",
    "            nodeNum += 1\n",
    "            nodeFeatures['otype'][nodeNum] = nodeType\n",
    "            for feat in feats:\n",
    "                nodeFeatures[feat][nodeNum] = feats[feat]\n",
    "            edgeFeatures['oslots'][nodeNum] = list(range(start, end + 1))\n",
    "    computeStatistics()\n",
    "    TF.save(nodeFeatures=nodeFeatures, edgeFeatures=edgeFeatures, metaData=metaData)\n",
    "    tm.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Generating text-fabric dataset\n",
      "  0.70s Computing statistics\n",
      "  1.38s Done\n",
      "  1.38s Adding statistics as features\n",
      "  1.62s Done\n",
      "  0.00s Exporting 10 node and 1 edge and 1 config features to /Users/dirk/github/text-fabric-data/sanskrit/dcs:\n",
      "   |     0.09s T book                 to /Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "   |     0.08s T book@sa              to /Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "   |     0.08s T chapter              to /Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "   |     2.14s T char                 to /Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "   |     0.24s T freq                 to /Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "   |     0.66s T otype                to /Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "   |     0.23s T rank                 to /Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "   |     2.07s T trailer              to /Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "   |     0.06s T verse                to /Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "   |     0.32s T word                 to /Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "   |     1.47s T oslots               to /Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "   |     0.00s M otext                to /Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "  7.46s Exported 10 node features and 1 edge features and 1 config features to /Users/dirk/github/text-fabric-data/sanskrit/dcs\n",
      "  9.09s Done\n"
     ]
    }
   ],
   "source": [
    "makeTextFabric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See the [tutorial](https://github.com/ETCBC/text-fabric/blob/master/docs/tutorialSanskrit.ipynb)\n",
    "for getting started *using* the new text-fabric dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
