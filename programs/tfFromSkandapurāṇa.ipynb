{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/skanda.jpg\" width=\"400\"/>\n",
    "<img src=\"images/tf-small.png\" width=\"70\"/>\n",
    "\n",
    "# The Skandapurāṇa project\n",
    "\n",
    "We convert the Skandapurāṇa text to TF. \n",
    "\n",
    "They come from Peter Bisschop's [Skandapurāṇa project](https://www.universiteitleiden.nl/en/research/research-projects/humanities/the-skandapurāṇa-project#tab-1).\n",
    "\n",
    "We used the transliterated and the Devanāgarī representations of the texts as found [here](https://www.universiteitleiden.nl/en/research/research-projects/humanities/the-skandapurāṇa-project#tab-4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of the TF modeling\n",
    "\n",
    "We take the Devanāgarī-character as smallest unit, the slot.\n",
    "Character nodes are called `char`.\n",
    "The unicode representation of a char is stored in the feature `dchar`.\n",
    "\n",
    "If a letter is the last letter of a word, we set its feature `last` to a space,\n",
    "otherwise the empty string.\n",
    "\n",
    "Words are the maximal stretches of Devanāgarī-chars that do not contain a space.\n",
    "Word nodes have node type `word`.\n",
    "\n",
    "The unicode representation of a word is stored in the feature `dword`.\n",
    "\n",
    "The transliteration of a word is stored in the feature `tword`.\n",
    "\n",
    "The top-level sectional unit is the node type `text`, and corresponds to the contents of a single file.\n",
    "\n",
    "Nodes of type `text` have the following features:\n",
    "\n",
    "* `name`: the name of the text. Usually this is just the number, but in some text\n",
    "  some characters are appended to the number;\n",
    "* `number`: the integer corresponding to the first triplet of digits\n",
    "  after the `SP` at the start of each line;\n",
    "* `volume`: as given in the description of the project\n",
    "\n",
    "Texts are subdivided into `verse`s.\n",
    "\n",
    "Nodes of type `verse` have the following features:\n",
    "\n",
    "* `number`: the integer corresponding to the second triplet of digits \n",
    "  after the `SP` at the start of each line.\n",
    "  \n",
    "Verses are subdivided into `line`s.\n",
    "\n",
    "Nodes of type `line` have the following features:\n",
    "\n",
    "* `number`: the integer corresponding to the last digit\n",
    "  after the `SP` at the start of each line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import generic Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, re, collections\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the import of the Text-Fabric library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tf.fabric import Fabric\n",
    "from tf.timestamp import Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = Timestamp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "We use variables to point to the input directories and the output tf directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE = os.path.expanduser('~/github/Dans-labs/text-fabric-data/sanskrit/sp')\n",
    "ORIG = f'{BASE}/devanagari'\n",
    "TRANS = f'{BASE}/transliteration'\n",
    "TF_DIR = f'{BASE}/tf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We glean the volume membership from the project description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOLUMES = dict(\n",
    "  I=(1,25),\n",
    "  IIA=(26,30),\n",
    "  IIB=(31,52),\n",
    "  III=(53,69),\n",
    "  S=(167,'s'),\n",
    "  RA=(167, 'ra'),  \n",
    ")\n",
    "RAS = (1,5)\n",
    "\n",
    "# node types\n",
    "\n",
    "TEXT = 'text'\n",
    "VERSE = 'verse'\n",
    "LINE = 'line'\n",
    "WORD = 'word'\n",
    "CHAR = 'char'\n",
    "\n",
    "NODE_TYPES = f'''\n",
    "  {CHAR}\n",
    "  {WORD}\n",
    "  {LINE}\n",
    "  {VERSE}\n",
    "  {TEXT}\n",
    "'''.strip().split()\n",
    "\n",
    "# features\n",
    "\n",
    "OTYPE = 'otype'\n",
    "OSLOTS = 'oslots'\n",
    "VOLUME = 'volume'\n",
    "NAME = 'name'\n",
    "NUMBER = 'number'\n",
    "DWORD = 'dword'\n",
    "TWORD = 'tword'\n",
    "DCHAR = 'dchar'\n",
    "LAST = 'last'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File list\n",
    "\n",
    "We generate the list of files in the corpus from the configuration.\n",
    "The result is represented as a list of items, each item is\n",
    "a document number plus document name plus a file name with its transliterated text, plus\n",
    "a file name with its devanagari text.\n",
    "\n",
    "We also map the document names to volume labels.\n",
    "\n",
    "Beyond the numbered texts there a a few special texts: S and RA recensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFileList():\n",
    "  textVolume = {}\n",
    "  texts = []\n",
    "  for (vol, (n, x)) in VOLUMES.items():\n",
    "    if type(x) is int:\n",
    "      for i in range(n, x + 1):\n",
    "        fileStart = f'st{i:>03}'\n",
    "        name = f'{i:>03}'\n",
    "        texts.append((i, name, f'{fileStart}.txt', f'{fileStart}_d.txt'))\n",
    "        textVolume[name] = vol\n",
    "    else:\n",
    "      fileStart = f'st{n:>03}'\n",
    "      if x == 's':\n",
    "        name = f'{n}S'\n",
    "        texts.append((n, name, f'{fileStart}_{x}.txt', f'{fileStart}_{x}_d.txt'))\n",
    "        textVolume[name] = vol\n",
    "      else:\n",
    "        for i in range(RAS[0], RAS[1] + 1):\n",
    "          name = f'{n}RA{i}'\n",
    "          texts.append((n, name, f'{fileStart}_{x}{i}.txt', f'{fileStart}_{x}{i}_d.txt'))\n",
    "          textVolume[name] = vol\n",
    "  return (texts, textVolume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the corpus files\n",
    "\n",
    "For each text item in the list, we read the associated files from disk.\n",
    "The file contents is chopped up in lines, and the text-containing lines\n",
    "are chopped up in words.\n",
    "\n",
    "All this data is collected in one big data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readText(n, name, transFile, devFile):\n",
    "  good = True\n",
    "  transPath = f'{TRANS}/{transFile}'\n",
    "  devPath = f'{ORIG}/{devFile}'\n",
    "  results = []\n",
    "  for path in (transPath, devPath):\n",
    "    if not os.path.isfile(path):\n",
    "      tm.error(f'{name}: file does not exist: {path}')\n",
    "      good = False\n",
    "      continue\n",
    "    with open(path) as fh:\n",
    "      results.append([\n",
    "        line.rstrip('\\n').split('|', 1)[0].split()\n",
    "        for line in fh\n",
    "        if line.startswith('SP')\n",
    "      ])\n",
    "  return results if good else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCorpus(texts):\n",
    "  data = []\n",
    "  tm.indent(reset=True)\n",
    "  tm.info('Reading corpus')\n",
    "  for item in texts:\n",
    "    tm.info(f'\\t{item[1]}')\n",
    "    results = readText(*item)\n",
    "    if results is not None:\n",
    "      data.append((item[0], item[1], *results))\n",
    "  tm.info('Done')\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proto TF\n",
    "\n",
    "We process the data and compose the feature data for the `oslots` edge feature and\n",
    "the desired node features.\n",
    "\n",
    "In this stage we cannot know the eventual node numbers, so we identify each node by node type and node number within its type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protoTf(data, textVolume):\n",
    "  curSlot = 0\n",
    "  curWord = 0\n",
    "  curLine = 0\n",
    "  curVerse = 0\n",
    "  curText = 0\n",
    "  edgeFeatures = {\n",
    "    OSLOTS: {},\n",
    "  }\n",
    "  nodeFeatures = {\n",
    "    OTYPE: {},\n",
    "    VOLUME: {},\n",
    "    NAME: {},\n",
    "    NUMBER: {},\n",
    "    DWORD: {},\n",
    "    TWORD: {},\n",
    "    DCHAR: {},\n",
    "    LAST: {},\n",
    "  }\n",
    "\n",
    "  tm.indent(reset=True)\n",
    "  tm.info('Proto TF generation')\n",
    "  for (textNum, textName, transData, devData) in data:\n",
    "    tm.info(f'\\t{textName}')\n",
    "    curText += 1\n",
    "    nodeFeatures[OTYPE][(TEXT, curText)] = TEXT\n",
    "    nodeFeatures[NAME][(TEXT, curText)] = textName\n",
    "    nodeFeatures[NUMBER][(TEXT, curText)] = textNum\n",
    "    nodeFeatures[VOLUME][(TEXT, curText)] = textVolume[textName]\n",
    "    firstTextSlot = curSlot + 1\n",
    "    firstVerseSlot = curSlot + 1\n",
    "    firstLineSlot = curSlot + 1\n",
    "    verseNum = None\n",
    "    lineNum = None\n",
    "    for (i, dLine) in enumerate(devData):\n",
    "      label = dLine[0]\n",
    "      dWords = dLine[1:]\n",
    "      tWords = transData[i][1:]\n",
    "      labelNumbers = label[10:14] if label[2:4] == 'ra' else label[5:9]\n",
    "      thisVerseNum = int(labelNumbers[0:3])\n",
    "      thisLineNum = int(0 if labelNumbers[3] == ':' else labelNumbers[3])\n",
    "      if thisLineNum != lineNum or thisVerseNum != verseNum:\n",
    "        if lineNum is not None:\n",
    "          curLine += 1\n",
    "          nodeFeatures[OTYPE][(LINE, curLine)] = LINE\n",
    "          nodeFeatures[NUMBER][(LINE, curLine)] = lineNum\n",
    "          edgeFeatures[OSLOTS][(LINE, curLine)] = set(range(firstLineSlot, curSlot + 1))\n",
    "          firstLineSlot = curSlot + 1\n",
    "        lineNum = thisLineNum\n",
    "      if thisVerseNum != verseNum:\n",
    "        if verseNum is not None:\n",
    "          curVerse += 1\n",
    "          nodeFeatures[OTYPE][(VERSE, curVerse)] = VERSE\n",
    "          nodeFeatures[NUMBER][(VERSE, curVerse)] = verseNum\n",
    "          edgeFeatures[OSLOTS][(VERSE, curVerse)] = set(range(firstVerseSlot, curSlot + 1))\n",
    "          firstVerseSlot = curSlot + 1\n",
    "        verseNum = thisVerseNum\n",
    "      if thisLineNum != lineNum or thisVerseNum != verseNum:\n",
    "        if lineNum is not None:\n",
    "          curLine += 1\n",
    "          nodeFeatures[OTYPE][(LINE, curLine)] = LINE\n",
    "          nodeFeatures[NUMBER][(LINE, curLine)] = lineNum\n",
    "          edgeFeatures[OSLOTS][(LINE, curLine)] = set(range(firstLineSlot, curSlot + 1))\n",
    "          firstLineSlot = curSlot + 1\n",
    "        lineNum = thisLineNum\n",
    "      for (j, dWord) in enumerate(dWords):\n",
    "        tWord = tWords[j]\n",
    "        curWord += 1\n",
    "        nodeFeatures[OTYPE][(WORD, curWord)] = WORD\n",
    "        nodeFeatures[DWORD][(WORD, curWord)] = dWord\n",
    "        nodeFeatures[TWORD][(WORD, curWord)] = tWord\n",
    "        edgeFeatures[OSLOTS][(WORD, curWord)] = set(range(curSlot + 1, curSlot + 1 + len(dWord)))\n",
    "        for d in dWord:\n",
    "          curSlot += 1\n",
    "          nodeFeatures[OTYPE][(CHAR, curSlot)] = CHAR\n",
    "          nodeFeatures[DCHAR][(CHAR, curSlot)] = d\n",
    "          nodeFeatures[LAST][(CHAR, curSlot)] = ''\n",
    "        nodeFeatures[LAST][(CHAR, curSlot)] = ' '\n",
    "    if verseNum is not None:\n",
    "      curVerse += 1\n",
    "      nodeFeatures[OTYPE][(VERSE, curVerse)] = VERSE\n",
    "      nodeFeatures[NUMBER][(VERSE, curVerse)] = verseNum\n",
    "      edgeFeatures[OSLOTS][(VERSE, curVerse)] = set(range(firstVerseSlot, curSlot + 1))\n",
    "    if lineNum is not None:\n",
    "      curLine += 1\n",
    "      nodeFeatures[OTYPE][(LINE, curLine)] = LINE\n",
    "      nodeFeatures[NUMBER][(LINE, curLine)] = lineNum\n",
    "      edgeFeatures[OSLOTS][(LINE, curLine)] = set(range(firstLineSlot, curSlot + 1))\n",
    "    edgeFeatures[OSLOTS][(TEXT, curText)] = set(range(firstTextSlot, curSlot + 1))\n",
    "  tm.info('Done')\n",
    "  tm.info('Checking whether all slots are contained in a word, line, verse and text')\n",
    "  typeSlots = {}\n",
    "  for ((nType, n), slots) in edgeFeatures[OSLOTS].items():\n",
    "    typeSlots.setdefault(nType, set())\n",
    "    typeSlots[nType] |= slots\n",
    "  for (nType, slots) in typeSlots.items():\n",
    "    minSlot = min(slots)\n",
    "    maxSlot = max(slots)\n",
    "    ok = minSlot == 1 and maxSlot == curSlot and len(slots) == maxSlot\n",
    "    okRep = 'ok' if ok else '!!!'\n",
    "    print(f'{nType:<8}: {len(slots)} elements between {minSlot} - {maxSlot} ({okRep})')\n",
    "  return (nodeFeatures, edgeFeatures)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF features\n",
    "\n",
    "We create real TF feature data, by ordering all nodes into one big sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfFeatures(nodeFeaturesProto, edgeFeaturesProto):\n",
    "  errors = set()\n",
    "  nodeTypeSets = {}\n",
    "  for ((otp, n), xtp) in nodeFeaturesProto[OTYPE].items():\n",
    "    if otp != xtp:\n",
    "      errors.add((otp, n, xtp))\n",
    "    nodeTypeSets.setdefault(otp, set()).add(n)\n",
    "  print(f'{len(errors)} inconsistencies')\n",
    "  curOffset = 0\n",
    "  offsets = {}\n",
    "  for otp in NODE_TYPES:\n",
    "    offsets[otp] = curOffset\n",
    "    ns = nodeTypeSets[otp]\n",
    "    minNtp = min(ns)\n",
    "    if minNtp != 1:\n",
    "      print(f'Node type {otp} starts with {minNtp}')\n",
    "    maxNtp = max(ns)\n",
    "    if maxNtp != len(ns):\n",
    "      print(f'Node type {otp} has holes in the sequence: {len(ns)} vs {maxNtp}')\n",
    "    print(f'{otp:<8}: {maxNtp:>6}')\n",
    "    curOffset += maxNtp\n",
    "  nodeFeatures = {}\n",
    "  edgeFeatures = {}\n",
    "  for (feature, data) in nodeFeaturesProto.items():\n",
    "    featureData = {}\n",
    "    for ((ntp, n), value) in data.items():\n",
    "      featureData[n + offsets[ntp]] = value\n",
    "    nodeFeatures[feature] = featureData\n",
    "  for (feature, data) in edgeFeaturesProto.items():\n",
    "    featureData = {}\n",
    "    for ((ntp, n), value) in data.items():\n",
    "      featureData[n + offsets[ntp]] = value\n",
    "    edgeFeatures[feature] = featureData\n",
    "  for (otp, offset) in offsets.items():\n",
    "    print(f'{otp} has offset {offset}')\n",
    "    for i in range(max((offset - 3, 1)), offset + 4):\n",
    "      print(f'{i:>6} is a {nodeFeatures[\"otype\"][i]}')\n",
    "  return (nodeFeatures, edgeFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main steps\n",
    "\n",
    "Here we execute the main steps, as defined in the functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(texts, textVolume) = makeFileList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Reading corpus\n",
      "  0.00s \t001\n",
      "  0.01s \t002\n",
      "  0.01s \t003\n",
      "  0.02s \t004\n",
      "  0.02s \t005\n",
      "  0.03s \t006\n",
      "  0.03s \t007\n",
      "  0.04s \t008\n",
      "  0.04s \t009\n",
      "  0.05s \t010\n",
      "  0.05s \t011\n",
      "  0.05s \t012\n",
      "  0.06s \t013\n",
      "  0.07s \t014\n",
      "  0.07s \t015\n",
      "  0.07s \t016\n",
      "  0.08s \t017\n",
      "  0.08s \t018\n",
      "  0.09s \t019\n",
      "  0.09s \t020\n",
      "  0.10s \t021\n",
      "  0.10s \t022\n",
      "  0.11s \t023\n",
      "  0.11s \t024\n",
      "  0.12s \t025\n",
      "  0.12s \t026\n",
      "  0.13s \t027\n",
      "  0.13s \t028\n",
      "  0.14s \t029\n",
      "  0.15s \t030\n",
      "  0.15s \t031\n",
      "  0.16s \t032\n",
      "  0.17s \t033\n",
      "  0.18s \t034\n",
      "  0.18s \t035\n",
      "  0.19s \t036\n",
      "  0.19s \t037\n",
      "  0.20s \t038\n",
      "  0.21s \t039\n",
      "  0.21s \t040\n",
      "  0.21s \t041\n",
      "  0.22s \t042\n",
      "  0.22s \t043\n",
      "  0.23s \t044\n",
      "  0.23s \t045\n",
      "  0.24s \t046\n",
      "  0.24s \t047\n",
      "  0.24s \t048\n",
      "  0.25s \t049\n",
      "  0.25s \t050\n",
      "  0.26s \t051\n",
      "  0.26s \t052\n",
      "  0.27s \t053\n",
      "  0.27s \t054\n",
      "  0.28s \t055\n",
      "  0.28s \t056\n",
      "  0.29s \t057\n",
      "  0.29s \t058\n",
      "  0.30s \t059\n",
      "  0.30s \t060\n",
      "  0.31s \t061\n",
      "  0.31s \t062\n",
      "  0.32s \t063\n",
      "  0.32s \t064\n",
      "  0.33s \t065\n",
      "  0.33s \t066\n",
      "  0.34s \t067\n",
      "  0.34s \t068\n",
      "  0.35s \t069\n",
      "  0.35s \t167S\n",
      "  0.36s \t167RA1\n",
      "  0.36s \t167RA2\n",
      "  0.37s \t167RA3\n",
      "  0.37s \t167RA4\n",
      "  0.38s \t167RA5\n",
      "  0.38s Done\n"
     ]
    }
   ],
   "source": [
    "data = readCorpus(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Proto TF generation\n",
      "  0.00s \t001\n",
      "  0.01s \t002\n",
      "  0.02s \t003\n",
      "  0.03s \t004\n",
      "  0.04s \t005\n",
      "  0.06s \t006\n",
      "  0.06s \t007\n",
      "  0.10s \t008\n",
      "  0.11s \t009\n",
      "  0.12s \t010\n",
      "  0.13s \t011\n",
      "  0.14s \t012\n",
      "  0.16s \t013\n",
      "  0.19s \t014\n",
      "  0.20s \t015\n",
      "  0.21s \t016\n",
      "  0.22s \t017\n",
      "  0.22s \t018\n",
      "  0.23s \t019\n",
      "  0.24s \t020\n",
      "  0.26s \t021\n",
      "  0.28s \t022\n",
      "  0.28s \t023\n",
      "  0.31s \t024\n",
      "  0.32s \t025\n",
      "  0.35s \t026\n",
      "  0.36s \t027\n",
      "  0.37s \t028\n",
      "  0.39s \t029\n",
      "  0.52s \t030\n",
      "  0.53s \t031\n",
      "  0.56s \t032\n",
      "  0.61s \t033\n",
      "  0.64s \t034\n",
      "  0.66s \t035\n",
      "  0.69s \t036\n",
      "  0.71s \t037\n",
      "  0.72s \t038\n",
      "  0.72s \t039\n",
      "  0.73s \t040\n",
      "  0.73s \t041\n",
      "  0.74s \t042\n",
      "  0.74s \t043\n",
      "  0.75s \t044\n",
      "  0.75s \t045\n",
      "  0.76s \t046\n",
      "  0.76s \t047\n",
      "  0.77s \t048\n",
      "  0.77s \t049\n",
      "  0.78s \t050\n",
      "  0.79s \t051\n",
      "  0.80s \t052\n",
      "  0.97s \t053\n",
      "  0.98s \t054\n",
      "  0.99s \t055\n",
      "  1.00s \t056\n",
      "  1.03s \t057\n",
      "  1.05s \t058\n",
      "  1.05s \t059\n",
      "  1.06s \t060\n",
      "  1.08s \t061\n",
      "  1.09s \t062\n",
      "  1.12s \t063\n",
      "  1.13s \t064\n",
      "  1.15s \t065\n",
      "  1.17s \t066\n",
      "  1.18s \t067\n",
      "  1.20s \t068\n",
      "  1.21s \t069\n",
      "  1.25s \t167S\n",
      "  1.29s \t167RA1\n",
      "  1.30s \t167RA2\n",
      "  1.31s \t167RA3\n",
      "  1.34s \t167RA4\n",
      "  1.35s \t167RA5\n",
      "  1.38s Done\n",
      "  1.38s Checking whether all slots are contained in a word, line, verse and text\n",
      "word    : 347770 elements between 1 - 347770 (ok)\n",
      "line    : 347770 elements between 1 - 347770 (ok)\n",
      "verse   : 347770 elements between 1 - 347770 (ok)\n",
      "text    : 347770 elements between 1 - 347770 (ok)\n"
     ]
    }
   ],
   "source": [
    "(nodeFeaturesProto, edgeFeaturesProto) = protoTf(data, textVolume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 inconsistencies\n",
      "char    : 347770\n",
      "word    :  45101\n",
      "line    :  10012\n",
      "verse   :   4532\n",
      "text    :     75\n",
      "char has offset 0\n",
      "     1 is a char\n",
      "     2 is a char\n",
      "     3 is a char\n",
      "word has offset 347770\n",
      "347767 is a char\n",
      "347768 is a char\n",
      "347769 is a char\n",
      "347770 is a char\n",
      "347771 is a word\n",
      "347772 is a word\n",
      "347773 is a word\n",
      "line has offset 392871\n",
      "392868 is a word\n",
      "392869 is a word\n",
      "392870 is a word\n",
      "392871 is a word\n",
      "392872 is a line\n",
      "392873 is a line\n",
      "392874 is a line\n",
      "verse has offset 402883\n",
      "402880 is a line\n",
      "402881 is a line\n",
      "402882 is a line\n",
      "402883 is a line\n",
      "402884 is a verse\n",
      "402885 is a verse\n",
      "402886 is a verse\n",
      "text has offset 407415\n",
      "407412 is a verse\n",
      "407413 is a verse\n",
      "407414 is a verse\n",
      "407415 is a verse\n",
      "407416 is a text\n",
      "407417 is a text\n",
      "407418 is a text\n"
     ]
    }
   ],
   "source": [
    "(nodeFeatures, edgeFeatures) = tfFeatures(nodeFeaturesProto, edgeFeaturesProto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "We supply the necessary metadata for the new features.\n",
    "We also have a few generic fields that will be added to all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metaData = {\n",
    "  '': dict(\n",
    "    createdBy='Peter Bisschop et al.',\n",
    "    convertedBy='Dirk Roorda',\n",
    "    name='Adhyāyas',\n",
    "    title='Skandapurāṇa Project',\n",
    "    source1='http://hum2.leidenuniv.nl/pdf/skandapurana-project/SP_all_devanagari.zip',\n",
    "    source2='http://hum2.leidenuniv.nl/pdf/skandapurana-project/SP_all_transliteration.zip',\n",
    "    provenance='https://www.universiteitleiden.nl/en/research/research-projects/humanities/the-skandapurāṇa-project',\n",
    "    description='volumes I-III of the critical edition of the Skandapurāṇa online',\n",
    "  ),\n",
    "  'otext': {\n",
    "    'sectionFeatures': ','.join((NAME, NUMBER, NUMBER)),\n",
    "    'sectionTypes': ','.join((TEXT, VERSE, LINE)),\n",
    "    'fmt:text-orig-full': f'{{{DCHAR}}}{{{LAST}}}',\n",
    "    'fmt:text-orig-word': f'{{{DWORD}}} ',\n",
    "    'fmt:text-trans-word': f'{{{TWORD}}} ',\n",
    "  },\n",
    "  'name@en': {\n",
    "    'valueType': 'str',\n",
    "    'language': 'english',\n",
    "    'languageCode': 'en',\n",
    "    'languageEnglish': 'English',\n",
    "  },\n",
    "}\n",
    "nodeFeatures['name@en'] = nodeFeatures[NAME]\n",
    "\n",
    "for feat in (OSLOTS, OTYPE, NAME, DWORD, TWORD, DCHAR, LAST, VOLUME):\n",
    "  metaData.setdefault(feat, {})['valueType'] = 'str'\n",
    "for feat in (NUMBER,):\n",
    "  metaData.setdefault(feat, {})['valueType'] = 'int'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data as TF data set\n",
    "\n",
    "The TF package has a function by which we can save all data that we have composed\n",
    "into a valid TF data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 5.5.18\n",
      "Api reference : https://dans-labs.github.io/text-fabric/Api/General/\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "11 features found and 0 ignored\n",
      "  0.00s Exporting 9 node and 1 edge and 1 config features to /Users/dirk/github/Dans-labs/text-fabric-data/sanskrit/sp/tf:\n",
      "   |     0.55s T dchar                to /Users/dirk/github/Dans-labs/text-fabric-data/sanskrit/sp/tf\n",
      "   |     0.08s T dword                to /Users/dirk/github/Dans-labs/text-fabric-data/sanskrit/sp/tf\n",
      "   |     0.59s T last                 to /Users/dirk/github/Dans-labs/text-fabric-data/sanskrit/sp/tf\n",
      "   |     0.00s T name                 to /Users/dirk/github/Dans-labs/text-fabric-data/sanskrit/sp/tf\n",
      "   |     0.00s T name@en              to /Users/dirk/github/Dans-labs/text-fabric-data/sanskrit/sp/tf\n",
      "   |     0.03s T number               to /Users/dirk/github/Dans-labs/text-fabric-data/sanskrit/sp/tf\n",
      "   |     0.19s T otype                to /Users/dirk/github/Dans-labs/text-fabric-data/sanskrit/sp/tf\n",
      "   |     0.09s T tword                to /Users/dirk/github/Dans-labs/text-fabric-data/sanskrit/sp/tf\n",
      "   |     0.00s T volume               to /Users/dirk/github/Dans-labs/text-fabric-data/sanskrit/sp/tf\n",
      "   |     0.37s T oslots               to /Users/dirk/github/Dans-labs/text-fabric-data/sanskrit/sp/tf\n",
      "   |     0.00s M otext                to /Users/dirk/github/Dans-labs/text-fabric-data/sanskrit/sp/tf\n",
      "  1.91s Exported 9 node features and 1 edge features and 1 config features to /Users/dirk/github/Dans-labs/text-fabric-data/sanskrit/sp/tf\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations=[TF_DIR])\n",
    "TF.save(nodeFeatures=nodeFeatures, edgeFeatures=edgeFeatures, metaData=metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with TF\n",
    "\n",
    "See the [tutorial](Skandapurāṇa.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
