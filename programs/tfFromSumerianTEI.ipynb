{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF from Sumerian\n",
    "\n",
    "We convert a Sumerian dataset, indicated by Justin Cale Johnson, to text-fabric format.\n",
    "\n",
    "## Model\n",
    "\n",
    "We divide the corpus in sections as follows:\n",
    "\n",
    "* **compositions** Correspond to the individual files;\n",
    "* **sections** Correspond to consecutive `<l>` elements with the same `corresp` attribute;\n",
    "* **lines** Correspond to the individual `<l>` elements.\n",
    "\n",
    "So much for the sectioning.\n",
    "The text is divided further as follows:\n",
    "\n",
    "* **words** Correspond to the individual `<w>` elements;\n",
    "* **glyphs** Correspond to the `-` separated chunks that constitute words.\n",
    "\n",
    "All these divisions are exactly the node types of the resulting TF dataset.\n",
    "The slot type is `glyph`.\n",
    "\n",
    "### NB 1:\n",
    "Words may contain substrings of the form `&`*xyz*`;`.\n",
    "This *xyz* is either an HTML entity that stands for an unicode character.\n",
    "In those cases we replace the entity by the corresponding unicode character.\n",
    "\n",
    "In other cases we consider the *xyz* also to be a glyph, and we translate it into `{`*xyz*`}`.\n",
    "\n",
    "### NB 2:\n",
    "Sometimes lines or sections are empty, i.e. there is no concrete glyph in it.\n",
    "This does not play nice with TF, so we add a single, empty glyph in those elements.\n",
    "\n",
    "## Coverage\n",
    "The `<w>` elements may contain several types of elements. We have only covered\n",
    "**corr corrEnd damage damageEnd supplied suppliedEnd**, and we ignore (for the moment)\n",
    "**gloss note term unclear**.\n",
    "\n",
    "There are also elements between the `<w>` elements, such as `<distinct>`.\n",
    "These we have ignored (so far).\n",
    "\n",
    "Most information in the `<teiHeader>` we ignore,\n",
    "except the `<title>` in `<fileDesc><titleStmt>`.\n",
    "\n",
    "### Notes on features\n",
    "\n",
    "#### text-fabric specific\n",
    "\n",
    "* **otype** for each node type (such as `composition`, `section`, `word`, etc), lists\n",
    "  the ranges of nodes that are member of that type\n",
    "* **oslots** for each node (text-object), lists the glyph positions that are part of it\n",
    "* **otext** configures the sections (`composition`, `section`, `line`) and defines\n",
    "  text rendering formats.\n",
    "\n",
    "#### composition\n",
    "\n",
    "* **compNum** the hierarchical number of the composition, as found in the file name\n",
    "* **title** the English title of the composition, as found in the TEI header\n",
    "\n",
    "#### section\n",
    "\n",
    "* **secNum** the number of the section, as found in the `corresp` attribute on the `<l>`\n",
    "  elements. We take the part after the `p`, and omit the rest. This is always a number.\n",
    "  If the `corresp` attribute is missing, we fill in the value 0.\n",
    "* **translation** the English translation of this section. See *Note on translations* below.\n",
    "  \n",
    "#### line\n",
    "\n",
    "* **lineNum** the number of the line, as found in the `n` attribute on the `<l>` \n",
    "  elements. This is not always a number.\n",
    "  \n",
    "#### word\n",
    "All attributes on the `<w>` elements are preserved under the same name:\n",
    "\n",
    "* **bound det emesal-prefix emesal form-type form label lemma npart pos type**\n",
    "* **freq_occ** computed feature with the frequency of each word form (using the\n",
    "  `form` attribute of the `<w>` element)\n",
    "* **freq_lex** computed feature with the frequency of each word lexeme (using the\n",
    "  `lemma` attribute of the `<w>` element)\n",
    "* **rank_occ rank_lex** derived from the corresponding `freq_` features.\n",
    "  A node with top frequency has rank 1, lesser frequencies get higher ranks.\n",
    "\n",
    "#### glyph\n",
    "\n",
    "* **ascii** the textual representation of the glyph, as found in the content of the `<w>`\n",
    "  elements.\n",
    "* **trailer** the material to put behind each glyph in order to recreate the original text.\n",
    "  In most cases, this will be a `-`. But for the last glyph of a word, it is ' '.\n",
    "  And if the glyph is of the form `{`*xyz*`}`, it is '', or '-', or ' ', depending on \n",
    "  where it is encountered.\n",
    "* **corr** comes from the `<corr>` and `<corrEnd>` elements. All glyphs inside a `<corr>` or\n",
    "  between a `<corr/>` and `<corrEnd/>` have value 1, the other glyphs have no value.\n",
    "* **damage** All glyphs between a `<damage/>` and `<damageEnd/>` have value 1, the other   \n",
    "  glyphs have no value.\n",
    "* **supplied** All glyphs between a `<supplied/>` and `<suppliedEnd/>` have value 1, the other   glyphs have no value.\n",
    "* **freq_occ** computed feature with the frequency of each glyph\n",
    "* **rank_occ** derived from the corresponding `freq_occ` feature.\n",
    "  A node with top frequency has rank 1, lesser frequencies get higher ranks.\n",
    "\n",
    "## Note on translations\n",
    "\n",
    "There are translations per section. We find them in the file `alldb.sql`. We use the table `simpletranslation`.\n",
    "The translation records there are linked to composition and section, which we use to make the match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude\n",
    "General imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os, collections, re, sqlite3\n",
    "import xml.etree.ElementTree as ET\n",
    "from html import unescape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the text-fabric package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tf.fabric import Fabric\n",
    "from tf.timestamp import Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.9\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "29 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "tm = Timestamp()\n",
    "TF = Fabric(locations='~/Dropbox/text-fabric-data', modules='sumerian/etcsl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the location of the source materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASEDIR = '~/Dropbox/text-fabric-data/sumerian/'.replace(\n",
    "    '~', os.path.expanduser('~').replace('\\\\', '/'),\n",
    ")\n",
    "SOURCE_TEI = '{}/etcsl-tei-source'.format(BASEDIR)\n",
    "SOURCE_SQL = '{}/etcsl-sql-source'.format(BASEDIR)\n",
    "\n",
    "TRANSLATIONS_FILE = '{}/simpletranslation.sql'.format(SOURCE_SQL)\n",
    "\n",
    "sectionpat = re.compile('^.*\\.p([0-9]+)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab the translations from the SQL file\n",
    "\n",
    "We use the table `simpletranslation`, not `translation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(':memory:')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 10: CREATE TABLE simpletranslation (  lineID\n",
      "Line 11: INSERT INTO simpletranslation VALUES (1,\n",
      "Line 12: INSERT INTO simpletranslation VALUES (19\n",
      "Line 13: INSERT INTO simpletranslation VALUES (44\n"
     ]
    }
   ],
   "source": [
    "with open(TRANSLATIONS_FILE) as sf:\n",
    "    sql = ''\n",
    "    for (ln, line) in enumerate(sf):\n",
    "        line = line.rstrip('\\n')\n",
    "        sql += line\n",
    "        if line.endswith(';'):\n",
    "            print('Line {}: {}'.format(ln + 1, sql[0:40]))\n",
    "            sql = sql.replace(\"\\\\'\", \"''\").replace('\\\\r', '').replace('\\\\\"', '\"')\n",
    "            c.execute(sql)\n",
    "            sql = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conn.cursor()\n",
    "rows = []\n",
    "for row in c.execute(\"SELECT * FROM simpletranslation\"):\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6508"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = dict()\n",
    "unmatched = collections.defaultdict(list)\n",
    "for (i, row) in enumerate(rows):\n",
    "    comp = row[4][1:].split('.', 1)[0]\n",
    "    translation = row[6]\n",
    "    sectionFull = row[3]\n",
    "    match = sectionpat.findall(sectionFull)\n",
    "    if not match:\n",
    "        unmatched[translation].append(rows[i])\n",
    "        continue\n",
    "    section = match[0]\n",
    "    translations[(comp, section)] = translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'(1 line fragmentary) (1 line missing)': [(1386,\n",
       "               't.2.2.3',\n",
       "               'The lament for Sumer and Urim',\n",
       "               'x',\n",
       "               'x',\n",
       "               'x',\n",
       "               '(1 line fragmentary) (1 line missing)')],\n",
       "             '(This composition is inscribed on a tablet whose colophon specifies it as a &c;ir-nam&c;ub of Utu)': [(3824,\n",
       "               't.4.32.e',\n",
       "               'A &c;ir-nam&c;ub to Utu (Utu E)',\n",
       "               't432e.n1',\n",
       "               'x',\n",
       "               'x',\n",
       "               '(This composition is inscribed on a tablet whose colophon specifies it as a &c;ir-nam&c;ub of Utu)'),\n",
       "              (3832,\n",
       "               't.4.32.f',\n",
       "               'A &c;ir-nam&c;ub to Utu (Utu F)',\n",
       "               't432f.n1',\n",
       "               'x',\n",
       "               'x',\n",
       "               '(This composition is inscribed on a tablet whose colophon specifies it as a &c;ir-nam&c;ub of Utu)')],\n",
       "             '(unknown no. of lines missing)': [(1915,\n",
       "               't.2.4.2.21',\n",
       "               'An adab (?) to Nergal for &C;ulgi (?) (&C;ulgi U)',\n",
       "               'x',\n",
       "               'x',\n",
       "               'x',\n",
       "               '(unknown no. of lines missing)'),\n",
       "              (2101,\n",
       "               't.2.5.1.2',\n",
       "               'I&c;bi-Erra and Kindattu (I&c;bi-Erra B)',\n",
       "               'x',\n",
       "               'x',\n",
       "               'x',\n",
       "               '(unknown no. of lines missing)')]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmatched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab the TEI data and store it in memory\n",
    "Set up an object in which all converted data is being collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.slotType = 'glyph'\n",
    "        self.slotNum = 0\n",
    "        self.nodeNum = 0\n",
    "        self.maxSlot = 0\n",
    "        self.maxNode = 0\n",
    "        self.paths = {}\n",
    "        self.slotFeatures = collections.defaultdict(dict)\n",
    "        self.nodeFeatures = collections.defaultdict(dict)\n",
    "        self.edgeSlotFeatures = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "        self.edgeFeatures = collections.defaultdict(lambda: collections.defaultdict(list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to read and convert the TEI XML of a single document (composition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sections = collections.defaultdict(dict)\n",
    "\n",
    "wordContentElems = set()\n",
    "\n",
    "spans = collections.defaultdict(list)\n",
    "\n",
    "def glyphsFromString(glyphString):\n",
    "    glyphs = []\n",
    "    glyphsMain = glyphString.split('-')\n",
    "    lastGlyphMain = len(glyphsMain) - 1\n",
    "    for (i, gm) in enumerate(glyphsMain):\n",
    "        glyphsSub = gm.split('}')\n",
    "        lastGlyphSub = len(glyphsSub) - 1\n",
    "        for (j, gs) in enumerate(glyphsSub):\n",
    "            glyphs.append(\n",
    "                (\n",
    "                    (gs + '}') if gs.startswith('{') else gs, \n",
    "                    ' ' if i == lastGlyphMain and j == lastGlyphSub else \\\n",
    "                    '-' if i != lastGlyphMain and j == lastGlyphSub else\\\n",
    "                    ''\n",
    "                )\n",
    "            )\n",
    "    return glyphs\n",
    "\n",
    "def doGlyphs(glyphString, compN, givenSecN, givenLineN, givenWordN):\n",
    "    glyphs = glyphsFromString(glyphString)\n",
    "    for (glyph, trailer) in glyphs:\n",
    "        data.slotNum += 1\n",
    "        glyphN = data.slotNum\n",
    "        data.slotFeatures['otype'][glyphN] = 'glyph'\n",
    "        data.slotFeatures['ascii'][glyphN] = glyph\n",
    "        data.slotFeatures['trailer'][glyphN] = trailer\n",
    "\n",
    "        data.edgeSlotFeatures['oslots'][compN].append(glyphN)\n",
    "        data.edgeSlotFeatures['oslots'][givenSecN].append(glyphN)\n",
    "        data.edgeSlotFeatures['oslots'][givenLineN].append(glyphN)\n",
    "        if givenWordN != None:\n",
    "            data.edgeSlotFeatures['oslots'][givenWordN].append(glyphN)\n",
    "\n",
    "def walkNode(node, path, compN, givenSecN=None, givenLineN=None, givenWordN=None):\n",
    "    secN = None\n",
    "    lineN = None\n",
    "    wordN = None\n",
    "    if node.tag == 'title' and path[-1] == 'titleStmt' and path[-2] == 'fileDesc' and path[-3] == 'teiHeader':\n",
    "        data.nodeFeatures['title'][compN] = ''.join(node.itertext())\n",
    "    elif node.tag == 'l':\n",
    "        if 'corresp' in node.attrib:\n",
    "            match = sectionpat.findall(node.attrib['corresp'])\n",
    "            secNum = match[0]\n",
    "        else:\n",
    "            secNum = '0'\n",
    "        if secNum not in sections[compN]:\n",
    "            data.nodeNum += 1\n",
    "            secN = data.nodeNum\n",
    "            data.nodeFeatures['otype'][secN] = 'section'\n",
    "            data.nodeFeatures['secNum'][secN] = secNum\n",
    "            compNum = data.nodeFeatures['compNum'][compN].replace('.','')\n",
    "            data.nodeFeatures['translation'][secN] = translations.get((compNum, secNum), 'X')\n",
    "            sections[compN][secNum] = secN\n",
    "        else:\n",
    "            secN = sections[compN][secNum]\n",
    "        data.nodeNum += 1\n",
    "        lineN = data.nodeNum\n",
    "        lineNum = node.attrib['n']\n",
    "        data.nodeFeatures['otype'][lineN] = 'line'\n",
    "        data.nodeFeatures['lineNum'][lineN] = lineNum\n",
    "        if node.find('.//w') == None:\n",
    "            data.slotNum += 1\n",
    "            glyphN = data.slotNum\n",
    "            data.slotFeatures['otype'][glyphN] = 'glyph'\n",
    "            data.slotFeatures['ascii'][glyphN] = ''\n",
    "            data.slotFeatures['trailer'][glyphN] = ''\n",
    "            data.edgeSlotFeatures['oslots'][compN].append(glyphN)\n",
    "            theSecN = secN if secN != None else givenSecN\n",
    "            data.edgeSlotFeatures['oslots'][theSecN].append(glyphN)\n",
    "            data.edgeSlotFeatures['oslots'][lineN].append(glyphN)\n",
    "    elif node.tag == 'w':\n",
    "        data.nodeNum += 1\n",
    "        wordN = data.nodeNum\n",
    "        data.nodeFeatures['otype'][wordN] = 'word'\n",
    "        for (att, val) in node.attrib.items():\n",
    "            data.nodeFeatures[att][wordN] = val\n",
    "        if node.text != None:\n",
    "            doGlyphs(node.text, compN, givenSecN, givenLineN, wordN)\n",
    "    elif node.tag in {'corr', 'damage', 'supplied'}:\n",
    "        spans[node.tag].append([data.slotNum + 1])\n",
    "        if node.text != None:\n",
    "            doGlyphs(node.text, compN, givenSecN, givenLineN, givenWordN)\n",
    "            spans[node.tag][-1].append(data.slotNum)\n",
    "    elif node.tag in {'corrEnd', 'damageEnd', 'suppliedEnd'}:\n",
    "        spans[node.tag.replace('End', '')][-1].append(data.slotNum)\n",
    "    if givenWordN != None:\n",
    "        wordContentElems.add(node.tag)\n",
    "        if node.text != None and node.tag not in {'corr', 'damage', 'supplied'}:\n",
    "            doGlyphs(node.text, compN, givenSecN, givenLineN, givenWordN)\n",
    "        if node.tail != None:\n",
    "            doGlyphs(node.tail, compN, givenSecN, givenLineN, givenWordN)\n",
    "        \n",
    "    theSecN = secN if secN != None else givenSecN\n",
    "    theLineN = lineN if lineN != None else givenLineN\n",
    "    theWordN = wordN if wordN != None else givenWordN\n",
    "    for child in node:\n",
    "        walkNode(\n",
    "            child, path + (node.tag,), compN,\n",
    "            givenSecN=theSecN, givenLineN=theLineN, givenWordN=theWordN,\n",
    "        )\n",
    "\n",
    "def getNode(root, compNum):\n",
    "    data.nodeNum += 1\n",
    "    compN = data.nodeNum\n",
    "    data.nodeFeatures['otype'][compN] = 'composition'\n",
    "    data.nodeFeatures['compNum'][compN] = compNum\n",
    "    walkNode(root, (), compN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to reorganize the data that has been collected, so that it is ready to be transformed to TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doSpans():\n",
    "    for (tag, stretches) in spans.items():\n",
    "        for span in stretches:\n",
    "            if len(span) < 2:\n",
    "                (start, end) = (span[0], span[0])\n",
    "            else:\n",
    "                (start, end) = (span[0], span[-1])\n",
    "            for glyphN in range(start, end + 1):\n",
    "                data.slotFeatures[tag][glyphN] = '1'\n",
    "\n",
    "def reorder():\n",
    "    slotType = data.slotType\n",
    "    data.maxSlot = data.slotNum\n",
    "    data.maxNode = data.nodeNum\n",
    "    otypeValues = set(data.nodeFeatures['otype'].values())\n",
    "    newIds = sorted(\n",
    "        range(1, data.maxNode + 1),\n",
    "        key=lambda n: (data.nodeFeatures['otype'][n], n),\n",
    "    )\n",
    "    mapping = dict(((v, i + 1 + data.maxSlot) for (i, v) in enumerate(newIds)))\n",
    "    \n",
    "    orderedFeatures = {}\n",
    "    for (name, dat) in data.nodeFeatures.items():\n",
    "        orderedFeatures[name] = dict(((mapping[n], v) for (n, v) in dat.items()))\n",
    "    for (name, dat) in data.slotFeatures.items():\n",
    "        if name not in orderedFeatures: orderedFeatures[name] = {}\n",
    "        orderedFeatures[name].update(dat)\n",
    "    data.nodeFeatures = orderedFeatures\n",
    "\n",
    "    orderedFeatures = {}\n",
    "    for (name, dat) in data.edgeFeatures.items():\n",
    "        orderedFeatures[name] = dict(((mapping[n], [mapping[m] for m in v]) for (n, v) in dat.items()))\n",
    "    for (name, dat) in data.edgeSlotFeatures.items():\n",
    "        if name not in orderedFeatures: orderedFeatures[name] = {}\n",
    "        orderedFeatures[name].update(dict(((mapping[n], v) for (n, v) in dat.items())))\n",
    "    data.edgeFeatures = orderedFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put everything together: \n",
    "\n",
    "* read the files\n",
    "* postprocess the data\n",
    "\n",
    "This will result in having all data in memory, in datastructures that can be readily written to TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Scanning TEI sources of all compositions\n",
      "   |     0.00s composition   0: 0.1.1\n",
      "   |     0.01s composition   1: 0.1.2\n",
      "   |     0.02s composition   2: 0.2.01\n",
      "   |     0.04s composition   3: 0.2.02\n",
      "   |     0.06s composition   4: 0.2.03\n",
      "   |     0.07s composition   5: 0.2.04\n",
      "   |     0.09s composition   6: 0.2.05\n",
      "   |     0.10s composition   7: 0.2.06\n",
      "   |     0.12s composition   8: 0.2.07\n",
      "   |     0.14s composition   9: 0.2.08\n",
      "   |     0.15s composition  10: 0.2.11\n",
      "   |     0.16s composition  11: 0.2.12\n",
      "   |     0.17s composition  12: 0.2.13\n",
      "   |     0.18s composition  13: 1.1.1\n",
      "   |     0.23s composition  14: 1.1.2\n",
      "   |     0.27s composition  15: 1.1.3\n",
      "   |     0.34s composition  16: 1.1.4\n",
      "   |     0.37s composition  17: 1.2.1\n",
      "   |     0.40s composition  18: 1.2.2\n",
      "   |     0.45s composition  19: 1.3.1\n",
      "   |     0.53s composition  20: 1.3.2\n",
      "   |     0.58s composition  21: 1.3.3\n",
      "   |     0.64s composition  22: 1.3.4\n",
      "   |     0.66s composition  23: 1.3.5\n",
      "   |     0.69s composition  24: 1.4.1.1\n",
      "   |     0.71s composition  25: 1.4.1.3\n",
      "   |     0.74s composition  26: 1.4.1\n",
      "   |     0.80s composition  27: 1.4.3\n",
      "   |     0.86s composition  28: 1.4.4\n",
      "   |     0.89s composition  29: 1.5.1\n",
      "   |     0.94s composition  30: 1.6.1\n",
      "   |     0.98s composition  31: 1.6.2\n",
      "   |     1.16s composition  32: 1.6.3\n",
      "   |     1.18s composition  33: 1.7.1\n",
      "   |     1.21s composition  34: 1.7.3\n",
      "   |     1.24s composition  35: 1.7.4\n",
      "   |     1.26s composition  36: 1.7.6\n",
      "   |     1.28s composition  37: 1.7.7\n",
      "   |     1.30s composition  38: 1.7.8\n",
      "   |     1.33s composition  39: 1.8.1.1\n",
      "   |     1.36s composition  40: 1.8.1.2\n",
      "   |     1.41s composition  41: 1.8.1.3\n",
      "   |     1.48s composition  42: 1.8.1.4\n",
      "   |     1.55s composition  43: 1.8.1.5.1\n",
      "   |     1.59s composition  44: 1.8.1.5\n",
      "   |     1.64s composition  45: 1.8.2.1\n",
      "   |     1.71s composition  46: 1.8.2.2\n",
      "   |     1.77s composition  47: 1.8.2.3\n",
      "   |     1.85s composition  48: 1.8.2.4\n",
      "   |     1.91s composition  49: 2.1.1\n",
      "   |     2.00s composition  50: 2.1.2\n",
      "   |     2.03s composition  51: 2.1.3\n",
      "   |     2.04s composition  52: 2.1.4\n",
      "   |     2.06s composition  53: 2.1.5\n",
      "   |     2.11s composition  54: 2.1.6\n",
      "   |     2.13s composition  55: 2.1.7\n",
      "   |     2.26s composition  56: 2.2.2\n",
      "   |     2.33s composition  57: 2.2.3\n",
      "   |     2.41s composition  58: 2.2.4\n",
      "   |     2.46s composition  59: 2.2.5\n",
      "   |     2.51s composition  60: 2.2.6\n",
      "   |     2.54s composition  61: 2.3.1\n",
      "   |     2.56s composition  62: 2.3.2\n",
      "   |     2.58s composition  63: 2.4.1.1\n",
      "   |     2.65s composition  64: 2.4.1.2\n",
      "   |     2.67s composition  65: 2.4.1.3\n",
      "   |     2.70s composition  66: 2.4.1.4\n",
      "   |     2.72s composition  67: 2.4.1.5\n",
      "   |     2.74s composition  68: 2.4.1.6\n",
      "   |     2.76s composition  69: 2.4.1.7\n",
      "   |     2.81s composition  70: 2.4.1.8\n",
      "   |     2.82s composition  71: 2.4.1.a\n",
      "   |     2.82s composition  72: 2.4.2.01\n",
      "   |     2.85s composition  73: 2.4.2.02\n",
      "   |     2.94s composition  74: 2.4.2.03\n",
      "   |     2.99s composition  75: 2.4.2.04\n",
      "   |     3.04s composition  76: 2.4.2.05\n",
      "   |     3.08s composition  77: 2.4.2.07\n",
      "   |     3.10s composition  78: 2.4.2.12\n",
      "   |     3.11s composition  79: 2.4.2.14\n",
      "   |     3.14s composition  80: 2.4.2.15\n",
      "   |     3.17s composition  81: 2.4.2.16\n",
      "   |     3.19s composition  82: 2.4.2.17\n",
      "   |     3.21s composition  83: 2.4.2.18\n",
      "   |     3.24s composition  84: 2.4.2.20\n",
      "   |     3.26s composition  85: 2.4.2.21\n",
      "   |     3.27s composition  86: 2.4.2.22\n",
      "   |     3.28s composition  87: 2.4.2.23\n",
      "   |     3.29s composition  88: 2.4.2.24\n",
      "   |     3.33s composition  89: 2.4.2.25\n",
      "   |     3.34s composition  90: 2.4.2.26\n",
      "   |     3.36s composition  91: 2.4.2.a\n",
      "   |     3.37s composition  92: 2.4.2.b\n",
      "   |     3.39s composition  93: 2.4.3.1\n",
      "   |     3.40s composition  94: 2.4.4.1\n",
      "   |     3.42s composition  95: 2.4.4.2\n",
      "   |     3.43s composition  96: 2.4.4.3\n",
      "   |     3.44s composition  97: 2.4.4.4\n",
      "   |     3.47s composition  98: 2.4.4.5\n",
      "   |     3.47s composition  99: 2.4.4.6\n",
      "   |     3.49s composition 100: 2.4.4.9\n",
      "   |     3.50s composition 101: 2.4.4.a\n",
      "   |     3.52s composition 102: 2.4.5.1\n",
      "   |     3.53s composition 103: 2.4.5.2\n",
      "   |     3.56s composition 104: 2.4.5.3\n",
      "   |     3.58s composition 105: 2.4.5.4\n",
      "   |     3.60s composition 106: 2.4.5.5\n",
      "   |     3.61s composition 107: 2.5.1.2\n",
      "   |     3.63s composition 108: 2.5.1.3\n",
      "   |     3.65s composition 109: 2.5.1.4\n",
      "   |     3.66s composition 110: 2.5.2.1\n",
      "   |     3.68s composition 111: 2.5.2.3\n",
      "   |     3.68s composition 112: 2.5.3.1\n",
      "   |     3.73s composition 113: 2.5.3.2\n",
      "   |     3.75s composition 114: 2.5.3.3\n",
      "   |     3.77s composition 115: 2.5.3.4\n",
      "   |     3.80s composition 116: 2.5.4.01\n",
      "   |     3.86s composition 117: 2.5.4.02\n",
      "   |     3.89s composition 118: 2.5.4.03\n",
      "   |     3.90s composition 119: 2.5.4.04\n",
      "   |     3.92s composition 120: 2.5.4.05\n",
      "   |     3.94s composition 121: 2.5.4.08\n",
      "   |     3.95s composition 122: 2.5.4.09\n",
      "   |     3.98s composition 123: 2.5.4.10\n",
      "   |     3.99s composition 124: 2.5.4.11\n",
      "   |     4.01s composition 125: 2.5.4.13\n",
      "   |     4.03s composition 126: 2.5.4.15\n",
      "   |     4.06s composition 127: 2.5.4.16\n",
      "   |     4.07s composition 128: 2.5.4.17\n",
      "   |     4.09s composition 129: 2.5.4.19\n",
      "   |     4.11s composition 130: 2.5.4.21\n",
      "   |     4.12s composition 131: 2.5.4.23\n",
      "   |     4.14s composition 132: 2.5.4.24\n",
      "   |     4.16s composition 133: 2.5.4.27\n",
      "   |     4.17s composition 134: 2.5.4.29\n",
      "   |     4.18s composition 135: 2.5.4.a\n",
      "   |     4.20s composition 136: 2.5.4.b\n",
      "   |     4.21s composition 137: 2.5.5.1\n",
      "   |     4.23s composition 138: 2.5.5.2\n",
      "   |     4.30s composition 139: 2.5.5.3\n",
      "   |     4.32s composition 140: 2.5.5.4\n",
      "   |     4.34s composition 141: 2.5.5.5\n",
      "   |     4.35s composition 142: 2.5.5.8\n",
      "   |     4.36s composition 143: 2.5.6.1\n",
      "   |     4.39s composition 144: 2.5.6.2\n",
      "   |     4.42s composition 145: 2.5.6.3\n",
      "   |     4.44s composition 146: 2.5.6.4\n",
      "   |     4.45s composition 147: 2.5.6.5\n",
      "   |     4.47s composition 148: 2.5.6.6\n",
      "   |     4.49s composition 149: 2.5.7.1\n",
      "   |     4.51s composition 150: 2.5.7.2\n",
      "   |     4.52s composition 151: 2.5.8.1\n",
      "   |     4.54s composition 152: 2.6.2.1\n",
      "   |     4.56s composition 153: 2.6.2.a\n",
      "   |     4.57s composition 154: 2.6.6.1\n",
      "   |     4.59s composition 155: 2.6.6.5\n",
      "   |     4.60s composition 156: 2.6.7.1\n",
      "   |     4.63s composition 157: 2.6.9.1\n",
      "   |     4.64s composition 158: 2.6.9.2\n",
      "   |     4.66s composition 159: 2.6.9.3\n",
      "   |     4.68s composition 160: 2.6.9.4\n",
      "   |     4.69s composition 161: 2.6.9.5\n",
      "   |     4.72s composition 162: 2.6.9.6\n",
      "   |     4.73s composition 163: 2.6.9.7\n",
      "   |     4.75s composition 164: 2.6.9.8\n",
      "   |     4.76s composition 165: 2.6.9.a\n",
      "   |     4.77s composition 166: 2.7.1.1\n",
      "   |     4.79s composition 167: 2.8.2.1\n",
      "   |     4.81s composition 168: 2.8.2.2\n",
      "   |     4.82s composition 169: 2.8.2.3\n",
      "   |     4.83s composition 170: 2.8.2.4\n",
      "   |     4.85s composition 171: 2.8.2.5\n",
      "   |     4.85s composition 172: 2.8.2.6\n",
      "   |     4.86s composition 173: 2.8.3.1\n",
      "   |     4.88s composition 174: 2.8.3.2\n",
      "   |     4.90s composition 175: 2.8.3.3\n",
      "   |     4.91s composition 176: 2.8.3.4\n",
      "   |     4.91s composition 177: 2.8.3.5\n",
      "   |     4.93s composition 178: 2.8.3.6\n",
      "   |     4.95s composition 179: 2.8.3.7\n",
      "   |     4.95s composition 180: 2.8.3.8\n",
      "   |     4.96s composition 181: 2.8.5.1\n",
      "   |     4.97s composition 182: 2.8.5.a\n",
      "   |     4.98s composition 183: 2.8.5.b\n",
      "   |     5.00s composition 184: 2.99.b\n",
      "   |     5.00s composition 185: 2.99.c\n",
      "   |     5.01s composition 186: 2.99.d\n",
      "   |     5.02s composition 187: 3.1.01\n",
      "   |     5.03s composition 188: 3.1.02\n",
      "   |     5.05s composition 189: 3.1.03\n",
      "   |     5.06s composition 190: 3.1.04\n",
      "   |     5.07s composition 191: 3.1.05\n",
      "   |     5.08s composition 192: 3.1.06.1\n",
      "   |     5.09s composition 193: 3.1.06\n",
      "   |     5.10s composition 194: 3.1.07\n",
      "   |     5.12s composition 195: 3.1.08\n",
      "   |     5.14s composition 196: 3.1.10\n",
      "   |     5.15s composition 197: 3.1.11.1\n",
      "   |     5.16s composition 198: 3.1.11\n",
      "   |     5.17s composition 199: 3.1.13.1\n",
      "   |     5.18s composition 200: 3.1.13.2\n",
      "   |     5.19s composition 201: 3.1.15\n",
      "   |     5.21s composition 202: 3.1.16\n",
      "   |     5.22s composition 203: 3.1.17\n",
      "   |     5.24s composition 204: 3.1.18\n",
      "   |     5.25s composition 205: 3.1.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     5.27s composition 206: 3.1.20\n",
      "   |     5.29s composition 207: 3.1.21\n",
      "   |     5.31s composition 208: 3.2.01\n",
      "   |     5.31s composition 209: 3.2.02\n",
      "   |     5.32s composition 210: 3.2.03\n",
      "   |     5.33s composition 211: 3.2.04\n",
      "   |     5.34s composition 212: 3.2.05\n",
      "   |     5.36s composition 213: 3.3.01\n",
      "   |     5.37s composition 214: 3.3.02\n",
      "   |     5.39s composition 215: 3.3.03\n",
      "   |     5.41s composition 216: 3.3.04\n",
      "   |     5.41s composition 217: 3.3.05\n",
      "   |     5.43s composition 218: 3.3.06\n",
      "   |     5.43s composition 219: 3.3.07\n",
      "   |     5.44s composition 220: 3.3.08\n",
      "   |     5.45s composition 221: 3.3.09\n",
      "   |     5.47s composition 222: 3.3.10\n",
      "   |     5.48s composition 223: 3.3.11\n",
      "   |     5.49s composition 224: 3.3.12\n",
      "   |     5.49s composition 225: 3.3.20\n",
      "   |     5.50s composition 226: 3.3.21\n",
      "   |     5.51s composition 227: 3.3.22\n",
      "   |     5.52s composition 228: 3.3.27\n",
      "   |     5.53s composition 229: 3.3.39\n",
      "   |     5.54s composition 230: 4.01.1\n",
      "   |     5.55s composition 231: 4.02.1\n",
      "   |     5.58s composition 232: 4.03.1\n",
      "   |     5.59s composition 233: 4.05.1\n",
      "   |     5.63s composition 234: 4.06.1\n",
      "   |     5.66s composition 235: 4.07.1\n",
      "   |     5.68s composition 236: 4.07.2\n",
      "   |     5.71s composition 237: 4.07.3\n",
      "   |     5.76s composition 238: 4.07.4\n",
      "   |     5.80s composition 239: 4.07.5\n",
      "   |     5.82s composition 240: 4.07.6\n",
      "   |     5.91s composition 241: 4.07.7\n",
      "   |     5.96s composition 242: 4.07.8\n",
      "   |     6.02s composition 243: 4.07.9\n",
      "   |     6.06s composition 244: 4.07.a\n",
      "   |     6.09s composition 245: 4.08.01\n",
      "   |     6.11s composition 246: 4.08.02\n",
      "   |     6.14s composition 247: 4.08.03\n",
      "   |     6.20s composition 248: 4.08.04\n",
      "   |     6.23s composition 249: 4.08.05\n",
      "   |     6.25s composition 250: 4.08.06\n",
      "   |     6.28s composition 251: 4.08.07\n",
      "   |     6.29s composition 252: 4.08.08\n",
      "   |     6.32s composition 253: 4.08.09\n",
      "   |     6.35s composition 254: 4.08.10\n",
      "   |     6.37s composition 255: 4.08.12\n",
      "   |     6.38s composition 256: 4.08.13\n",
      "   |     6.39s composition 257: 4.08.15\n",
      "   |     6.42s composition 258: 4.08.16\n",
      "   |     6.46s composition 259: 4.08.18\n",
      "   |     6.48s composition 260: 4.08.20\n",
      "   |     6.50s composition 261: 4.08.22\n",
      "   |     6.51s composition 262: 4.08.23\n",
      "   |     6.53s composition 263: 4.08.25\n",
      "   |     6.55s composition 264: 4.08.26\n",
      "   |     6.56s composition 265: 4.08.28\n",
      "   |     6.58s composition 266: 4.08.29\n",
      "   |     6.60s composition 267: 4.08.30\n",
      "   |     6.63s composition 268: 4.08.31\n",
      "   |     6.72s composition 269: 4.08.32\n",
      "   |     6.73s composition 270: 4.08.33\n",
      "   |     6.75s composition 271: 4.08.a\n",
      "   |     6.76s composition 272: 4.12.1\n",
      "   |     6.79s composition 273: 4.12.2\n",
      "   |     6.80s composition 274: 4.13.01\n",
      "   |     6.82s composition 275: 4.13.02\n",
      "   |     6.84s composition 276: 4.13.03\n",
      "   |     6.86s composition 277: 4.13.04\n",
      "   |     6.88s composition 278: 4.13.05\n",
      "   |     6.91s composition 279: 4.13.06\n",
      "   |     6.93s composition 280: 4.13.07\n",
      "   |     6.95s composition 281: 4.13.08\n",
      "   |     6.96s composition 282: 4.13.09\n",
      "   |     6.99s composition 283: 4.13.10\n",
      "   |     7.01s composition 284: 4.13.11\n",
      "   |     7.03s composition 285: 4.13.12\n",
      "   |     7.06s composition 286: 4.13.13\n",
      "   |     7.08s composition 287: 4.13.14\n",
      "   |     7.09s composition 288: 4.13.15\n",
      "   |     7.12s composition 289: 4.13.a\n",
      "   |     7.13s composition 290: 4.13.b\n",
      "   |     7.14s composition 291: 4.13.c\n",
      "   |     7.15s composition 292: 4.13.d\n",
      "   |     7.16s composition 293: 4.14.1\n",
      "   |     7.22s composition 294: 4.14.2\n",
      "   |     7.23s composition 295: 4.14.3\n",
      "   |     7.27s composition 296: 4.15.2\n",
      "   |     7.29s composition 297: 4.15.3\n",
      "   |     7.32s composition 298: 4.16.1\n",
      "   |     7.34s composition 299: 4.16.2\n",
      "   |     7.35s composition 300: 4.17.1\n",
      "   |     7.38s composition 301: 4.19.1\n",
      "   |     7.40s composition 302: 4.19.2\n",
      "   |     7.41s composition 303: 4.19.3\n",
      "   |     7.43s composition 304: 4.19.4\n",
      "   |     7.44s composition 305: 4.21.1\n",
      "   |     7.46s composition 306: 4.22.1\n",
      "   |     7.49s composition 307: 4.22.2\n",
      "   |     7.50s composition 308: 4.22.3\n",
      "   |     7.51s composition 309: 4.22.4\n",
      "   |     7.53s composition 310: 4.22.5\n",
      "   |     7.55s composition 311: 4.22.6\n",
      "   |     7.57s composition 312: 4.23.1\n",
      "   |     7.59s composition 313: 4.24.1\n",
      "   |     7.61s composition 314: 4.25.1\n",
      "   |     7.62s composition 315: 4.25.2\n",
      "   |     7.64s composition 316: 4.26.1\n",
      "   |     7.66s composition 317: 4.27.01\n",
      "   |     7.68s composition 318: 4.27.02\n",
      "   |     7.71s composition 319: 4.27.03\n",
      "   |     7.74s composition 320: 4.27.04\n",
      "   |     7.75s composition 321: 4.27.06\n",
      "   |     7.77s composition 322: 4.27.07\n",
      "   |     7.80s composition 323: 4.27.a\n",
      "   |     7.80s composition 324: 4.28.1\n",
      "   |     7.84s composition 325: 4.29.1\n",
      "   |     7.87s composition 326: 4.29.2\n",
      "   |     7.89s composition 327: 4.30.1\n",
      "   |     7.91s composition 328: 4.31.1\n",
      "   |     7.94s composition 329: 4.32.2\n",
      "   |     7.95s composition 330: 4.32.e\n",
      "   |     7.98s composition 331: 4.32.f\n",
      "   |     8.01s composition 332: 4.33.1\n",
      "   |     8.03s composition 333: 4.33.2\n",
      "   |     8.05s composition 334: 4.80.1\n",
      "   |     8.15s composition 335: 4.80.2\n",
      "   |     8.19s composition 336: 4.80.4\n",
      "   |     8.21s composition 337: 5.1.3\n",
      "   |     8.23s composition 338: 5.2.4\n",
      "   |     8.28s composition 339: 5.2.5\n",
      "   |     8.29s composition 340: 5.3.1\n",
      "   |     8.32s composition 341: 5.3.2\n",
      "   |     8.36s composition 342: 5.3.3\n",
      "   |     8.52s composition 343: 5.3.5\n",
      "   |     8.57s composition 344: 5.3.6\n",
      "   |     8.70s composition 345: 5.3.7\n",
      "   |     8.71s composition 346: 5.4.11\n",
      "   |     8.72s composition 347: 5.4.12\n",
      "   |     8.74s composition 348: 5.5.1\n",
      "   |     8.76s composition 349: 5.5.2\n",
      "   |     8.80s composition 350: 5.5.3\n",
      "   |     8.83s composition 351: 5.5.4\n",
      "   |     8.86s composition 352: 5.5.5\n",
      "   |     8.90s composition 353: 5.5.a\n",
      "   |     8.92s composition 354: 5.6.1\n",
      "   |     8.99s composition 355: 5.6.3\n",
      "   |     9.02s composition 356: 5.6.5\n",
      "   |     9.05s composition 357: 5.7.1\n",
      "   |     9.06s composition 358: 5.7.2\n",
      "   |     9.06s composition 359: 5.7.3\n",
      "   |     9.07s composition 360: 5.7.a\n",
      "   |     9.08s composition 361: 5.9.1\n",
      "   |     9.11s composition 362: 5.9.2\n",
      "   |     9.15s composition 363: 6.1.01\n",
      "   |     9.21s composition 364: 6.1.02\n",
      "   |     9.26s composition 365: 6.1.03\n",
      "   |     9.32s composition 366: 6.1.04\n",
      "   |     9.34s composition 367: 6.1.05\n",
      "   |     9.38s composition 368: 6.1.07\n",
      "   |     9.41s composition 369: 6.1.08\n",
      "   |     9.44s composition 370: 6.1.09\n",
      "   |     9.46s composition 371: 6.1.10\n",
      "   |     9.47s composition 372: 6.1.11\n",
      "   |     9.50s composition 373: 6.1.12\n",
      "   |     9.52s composition 374: 6.1.13\n",
      "   |     9.55s composition 375: 6.1.14\n",
      "   |     9.57s composition 376: 6.1.15\n",
      "   |     9.59s composition 377: 6.1.16\n",
      "   |     9.61s composition 378: 6.1.17\n",
      "   |     9.62s composition 379: 6.1.18\n",
      "   |     9.63s composition 380: 6.1.19\n",
      "   |     9.66s composition 381: 6.1.21\n",
      "   |     9.68s composition 382: 6.1.22\n",
      "   |     9.71s composition 383: 6.1.23\n",
      "   |     9.72s composition 384: 6.1.24\n",
      "   |     9.73s composition 385: 6.1.25\n",
      "   |     9.76s composition 386: 6.1.26\n",
      "   |     9.95s composition 387: 6.1.27\n",
      "   |    10.00s composition 388: 6.1.28\n",
      "   |       10s composition 389: 6.2.1\n",
      "   |       10s composition 390: 6.2.2\n",
      "   |       10s composition 391: 6.2.3\n",
      "   |       10s composition 392: 6.2.4\n",
      "   |       10s composition 393: 6.2.5\n",
      "    10s Slots:        412192\n",
      "    10s Other nodes:  213201\n",
      "    10s Processing data ...\n",
      "    11s Done\n",
      "Elements found in word content:\n",
      "\tcorr\n",
      "\tcorrEnd\n",
      "\tdamage\n",
      "\tdamageEnd\n",
      "\tgloss\n",
      "\tnote\n",
      "\tsupplied\n",
      "\tsuppliedEnd\n",
      "\tterm\n",
      "\tunclear\n"
     ]
    }
   ],
   "source": [
    "filenamepat = re.compile('^c\\.([0-9a-z.]*)$')\n",
    "entitypat = re.compile('&([^; \\n]+);')\n",
    "\n",
    "def replaceEntity(match): return '{{{}}}'.format(match.group(1))\n",
    "\n",
    "data = Data()\n",
    "\n",
    "tm.indent(level=0, reset=True)\n",
    "tm.info('Scanning TEI sources of all compositions')\n",
    "tm.indent(level=1, reset=True)\n",
    "for (i, xmlfile) in enumerate(glob.glob(SOURCE_TEI+'/*.xml')):\n",
    "    (dirName, baseName) = os.path.split(xmlfile)\n",
    "    (fileName, extension) = os.path.splitext(baseName)\n",
    "    match = filenamepat.findall(fileName)\n",
    "    if len(match) == 0:\n",
    "        tm.error('unexpected file: \"{}\"'.format(baseName))\n",
    "        continue\n",
    "    compNum = match[0]\n",
    "    tm.info('composition {:>3}: {}'.format(i, compNum))\n",
    "    with open(xmlfile) as xf:\n",
    "        text = unescape(xf.read())\n",
    "        text = entitypat.sub(replaceEntity, text)\n",
    "    root = ET.fromstring(text)\n",
    "    getNode(root, compNum)\n",
    "tm.indent(level=0)\n",
    "tm.info('Slots:       {:>7}'.format(data.slotNum))\n",
    "tm.info('Other nodes: {:>7}'.format(data.nodeNum))\n",
    "tm.info('Processing data ...')\n",
    "doSpans()\n",
    "reorder()\n",
    "tm.info('Done')\n",
    "\n",
    "print('Elements found in word content:\\n\\t{}'.format('\\n\\t'.join(sorted(wordContentElems))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few checks.\n",
    "Finding out the contents of the `<w>` elements was a matter of trial and error.\n",
    "It seems that there are still a few rough edges (very few, indeed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "damage   : 11306 spans,   1 faulty\n",
      "supplied : 17865 spans,   4 faulty\n",
      "corr     :  1102 spans,   0 faulty\n"
     ]
    }
   ],
   "source": [
    "faulty = collections.defaultdict(list)\n",
    "for (tag, stretches) in spans.items():\n",
    "    for span in stretches:\n",
    "        if len(span) != 2:\n",
    "            faulty[tag].append(span)\n",
    "    print('{:<9}: {:>5} spans, {:>3} faulty'.format(tag, len(stretches), len(faulty[tag])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'corr': [],\n",
       "             'damage': [[412032, 412032, 412032]],\n",
       "             'supplied': [[3176, 3180, 3186],\n",
       "              [157050],\n",
       "              [282876, 282877, 282885],\n",
       "              [396033]]})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Features: composition number\n",
    "\n",
    "For technical reasons we need a feature `compNum@en`, i.e. the composition number in English.\n",
    "It is identical to `compNum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.nodeFeatures['compNum@en'] = data.nodeFeatures['compNum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add metadata\n",
    "Before we can export to TF, we have to supply essential metadata about the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metaData = {\n",
    "    '': dict(\n",
    "        createdBy='Justin Cale Johnson and Dirk Roorda',\n",
    "    ),\n",
    "    'otext': {\n",
    "        'sectionFeatures': 'compNum,secNum,lineNum',\n",
    "        'sectionTypes': 'composition,section,line',\n",
    "        'fmt:text-orig-full': '{ascii}{trailer}',\n",
    "        'fmt:form-orig-full': '{form} ',\n",
    "        'fmt:lex-orig-full': '{lemma} ',\n",
    "        'fmt:gloss': '{label} ',\n",
    "        'fmt:translation': '{translation}\\\\n',\n",
    "    },\n",
    "}\n",
    "\n",
    "numberFeatures = set('''\n",
    "    secNum\n",
    "'''.strip().split())\n",
    "\n",
    "for nf in data.nodeFeatures:\n",
    "    metaData.setdefault(nf, {})['valueType'] = 'int' if nf in numberFeatures else 'str'\n",
    "for ef in data.edgeFeatures:\n",
    "    metaData.setdefault(ef, {})['valueType'] = 'int' if ef in numberFeatures else 'str'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features: statistical\n",
    "We add some statistical features:\n",
    "\n",
    "* rank and frequency for word occurrences and lexemes\n",
    "* rank and frequency for glyphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    12s Computing statistics\n",
      "    13s Done\n",
      "    13s Adding statistics as features\n",
      "    15s Done\n"
     ]
    }
   ],
   "source": [
    "tm.info('Computing statistics')\n",
    "gstats = {\n",
    "    'freq': {\n",
    "        'occ': collections.Counter(),\n",
    "    },\n",
    "    'rank': {\n",
    "        'occ': {},\n",
    "    },\n",
    "}\n",
    "wstats = {\n",
    "    'freq': {\n",
    "        'lex': collections.Counter(),\n",
    "        'occ': collections.Counter(),\n",
    "    },\n",
    "    'rank': {\n",
    "        'lex': {},\n",
    "        'occ': {},\n",
    "    },\n",
    "}\n",
    "\n",
    "nodeFeatures = data.nodeFeatures\n",
    "\n",
    "words = [n[0] for n in nodeFeatures['otype'].items() if n[1] == 'word']\n",
    "glyphs = [n[0] for n in nodeFeatures['otype'].items() if n[1] == 'glyph']\n",
    "\n",
    "for g in glyphs:\n",
    "    occ = nodeFeatures['ascii'][g]\n",
    "    gstats['freq']['occ'][occ] += 1\n",
    "tp = 'occ'\n",
    "rank = -1\n",
    "prev_n = -1\n",
    "amount = 1\n",
    "for (x, n) in sorted(gstats['freq'][tp].items(), key=lambda y: (-y[1], y[0])):\n",
    "    if n == prev_n:\n",
    "        amount += 1\n",
    "    else:\n",
    "        rank += amount\n",
    "        amount = 1\n",
    "    prev_n = n\n",
    "    gstats['rank'][tp][x] = rank\n",
    "        \n",
    "for w in words:\n",
    "    occ = nodeFeatures['form'][w]\n",
    "    lex = nodeFeatures['lemma'][w]\n",
    "    wstats['freq']['lex'][lex] += 1\n",
    "    wstats['freq']['occ'][occ] += 1\n",
    "for tp in ['lex', 'occ']:\n",
    "    rank = -1\n",
    "    prev_n = -1\n",
    "    amount = 1\n",
    "    for (x, n) in sorted(wstats['freq'][tp].items(), key=lambda y: (-y[1], y[0])):\n",
    "        if n == prev_n:\n",
    "            amount += 1\n",
    "        else:\n",
    "            rank += amount\n",
    "            amount = 1\n",
    "        prev_n = n\n",
    "        wstats['rank'][tp][x] = rank\n",
    "tm.info('Done')\n",
    "\n",
    "tm.info('Adding statistics as features')\n",
    "occFeatures = {}\n",
    "for tp in ['occ', 'lex']:\n",
    "    for ft in ('freq_{}'.format(tp), 'rank_{}'.format(tp)):\n",
    "        occFeatures[ft] = {}\n",
    "        metaData.setdefault(ft, {})['valueType'] = 'int'\n",
    "\n",
    "for g in glyphs:\n",
    "    occ = nodeFeatures['ascii'][g]\n",
    "    tp = 'occ'\n",
    "    ref = occ\n",
    "    for kn in ['freq', 'rank']:\n",
    "        ft = '{}_{}'.format(kn, tp)\n",
    "        occFeatures[ft][g] = str(gstats[kn][tp][ref])\n",
    "\n",
    "for w in words:\n",
    "    occ = nodeFeatures['form'][w]\n",
    "    lex = nodeFeatures['lemma'][w]\n",
    "    for tp in ['occ', 'lex']:\n",
    "        ref = occ if tp == 'occ' else lex\n",
    "        for kn in ['freq', 'rank']:\n",
    "            ft = '{}_{}'.format(kn, tp)\n",
    "            occFeatures[ft][w] = str(wstats[kn][tp][ref])\n",
    "\n",
    "nodeFeatures.update(occFeatures)\n",
    "\n",
    "tm.info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check whether some text objects have remained without glyphs\n",
    "(e.g. caused by a line with only a `<gap>` element and no `<w>` elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glyph: 1-412192\n",
      "composition: 412193-412586\n",
      "section: 449098-455609\n",
      "line: 412587-449097\n",
      "word: 455610-625393\n"
     ]
    }
   ],
   "source": [
    "for otype in ['glyph', 'composition', 'section', 'line', 'word']:\n",
    "    nodes = [n for n in data.nodeFeatures['otype'] if data.nodeFeatures['otype'][n] == otype]\n",
    "    print('{}: {}-{}'.format(\n",
    "        otype,\n",
    "        min(nodes),\n",
    "        max(nodes),\n",
    "    ))\n",
    "    if otype == 'glyph': continue\n",
    "    for n in nodes:\n",
    "        if n not in data.edgeFeatures['oslots']:\n",
    "            print('missing in {}: {}'.format(otype, n))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the TF dataset\n",
    "We can now produce the text-fabric dataset with one command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Exporting 27 node and 1 edge and 1 config features to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl:\n",
      "   |     0.84s T ascii                to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.01s T bound                to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.01s T compNum              to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.01s T compNum@en           to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.02s T corr                 to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.04s T damage               to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.04s T det                  to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.01s T emesal               to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.02s T emesal-prefix        to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.37s T form                 to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.03s T form-type            to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.41s T freq_lex             to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     1.50s T freq_occ             to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.47s T label                to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.35s T lemma                to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.08s T lineNum              to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.00s T npart                to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.37s T otype                to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.30s T pos                  to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.43s T rank_lex             to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     1.05s T rank_occ             to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.02s T secNum               to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.12s T supplied             to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.00s T title                to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.90s T trailer              to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.03s T translation          to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.05s T type                 to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     1.07s T oslots               to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "   |     0.00s M otext                to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n",
      "  8.61s Exported 27 node features and 1 edge features and 1 config features to /Users/dirk/Dropbox/text-fabric-data/sumerian/etcsl\n"
     ]
    }
   ],
   "source": [
    "TF.save(nodeFeatures=data.nodeFeatures, edgeFeatures=data.edgeFeatures, metaData=metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
